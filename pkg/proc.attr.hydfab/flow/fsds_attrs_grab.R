#' @title Processing script to grab catchment attributes
#' @author Guy Litt \code{guy.litt@noaa.gov}
#' @description
#' Given catchment location data generated by fsds_proc and a user-specified
#' configuration file for datasets and corresponding attributes of interest,
#' acquire the attributes for each catchment and write to file.
#' @seealso [fsds_proc] A python package that processes input data for the formulation-selector

# Changelog / Contributions
#   2024-07-24 Originally created, GL

library(tibble)
library(yaml)
library(tidync)
library(proc.attr.hydfab)

# TODO is AWS_NO_SIGN_REQUEST necessary??
# Sys.setenv(AWS_NO_SIGN_REQUEST="YES")

# TODO create config yaml.

# TODO read in config yaml, must populate NA for items that are empty.

# Define input directory:
# TODO change this to reading the standardized metadata, not the generated data
raw_config <- yaml::read_yaml("/Users/guylitt/git/fsds/scripts/eval_ingest/xssa/xssa_schema.yaml")

datasets <- 'all'

home_dir <- Sys.getenv("HOME")
dir_base <- file.path(home_dir,'noaa/regionalization/data')

dir_std_base <- file.path(dir_base,"input/user_data_std") # The location of standardized data generated by proc_fsds python package
dir_hydfab <- file.path(dir_base,'input','hydrofabric') # The local dir where hydrofabric data are stored to limit s3 connections
dir_db_attrs <- file.path(dir_base,'input','attributes') # The local dir where attribute data are stored for faster access


s3_base <- "s3://lynker-spatial/tabular-resources" # s3 path containing hydrofabric-formatted attribute datasets
s3_bucket <- 'lynker-spatial' # s3 bucket containing hydrofabric data


s3_path_hydatl <- glue::glue('{s3_base}/hydroATLAS/hydroatlas_vars.parquet') # path to hydroatlas data formatted for hydrofabric


# Additional config options
hf_cat_sel <- c("total","all")[1] # total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
ext <- 'gpkg'
attr_sources <- c("hydroatlas","usgs") # "streamcat",
ha_vars <- c('pet_mm_s01', 'cly_pc_sav', 'cly_pc_uav') # hydroatlas variables
sc_vars <- c() # TODO look up variables. May need to select datasets first
usgs_vars <- c('TOT_TWI','TOT_PRSNOW','TOT_POPDENS90','TOT_EWT','TOT_RECHG') # list of variables retrievable using nhdplusTools::get_characteristics_metadata()
#-----------------------------------------------------


# 'all' an option if processing all datasets desired. Otherwise list datasets in config file
if(datasets=='all'){
  datasets <- list.dirs(dir_std_base,recursive=F)
}

# TODO generate this listing structure based on what is provided in yaml config & accounting for empty entries
# Note that if a path is provided, ensure the variable name includes 'path'. Same for directory having variable name with 'dir'
Retr_Params <- list(paths = list(dir_hydfab=dir_hydfab,
                                 dir_db_attrs=dir_db_attrs,
                              s3_path_hydatl = s3_path_hydatl),
                 vars = list(usgs_vars = usgs_vars,
                             ha_vars = ha_vars)
                 )

# Path checker/maker of anything that's a directory
for(dir in Retr_Params$paths){
  if(base::grepl('dir',dir)){
    if(!base::dir.exists(dir)){
      base::dir.create(dir)
    }
  }
}


for (ds in datasets){
  ds <- "juliemai-xSSA" # TODO remove

  # TODO read in a standard format filename and file type
  dat_in <- file.path(dir_std_base,ds,'juliemai-xSSA_Raven_blended.nc')

  # TODO if reading a netcdf file:
  std_data <- tidync::tidync(dat_in)


  # TODO consider how different datasets will have different loc identifiers
  # Extract the gage ids
  gage_ids <- std_data$transforms$gage_id$gage_id
  featureSource <- 'nwissite' # TODO read in from std_data

  # Grab all needed attributes
  ls_gage_attr <- list()
  for (gid in gage_ids){
    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html


    site_id <- paste0('USGS-',gid) # TODO perform this in fsds_proc

    nldi_feat <- list(featureSource =featureSource,
                 featureID = site_id)
    site_feature <- nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat)
    comid <- site_feature['comid']$comid

    print(paste0("Processing COMID ",comid))
    # Retrieve the variables corresponding to datasets of interest
    all_attr <- proc.attr.hydfab::proc_attr_wrap(comid, Retr_Params,
                                                 lyrs='network',overwrite=FALSE)

    ls_gage_attr[[gid]] <- all_attr
  }

  # TODO Compile needed attributes from database with all the catchments of interest

  # TODO write combined dataset to file as a compiled collection for training/prediction

}
