#' @title Processing script to grab catchment attributes
#' @author Guy Litt \code{guy.litt@noaa.gov}
#' @description
#' Given catchment location data generated by fsds_proc and a user-specified
#' configuration file for datasets and corresponding attributes of interest,
#' acquire the attributes for each catchment, write to parquet database.
#' @details Builds a parquet database for each individual comid when calling
#' \code{proc.attr.hydfab::proc_attr_wrap}. From the directory of comids,
#' creates a sub-directory of a specific dataset
#' @seealso [fsds_proc] A python package that processes input data for the
#' formulation-selector

# Changelog / Contributions
#   2024-07-24 Originally created, GL

library(tibble)
library(yaml)
library(tidync)
library(proc.attr.hydfab)
library(glue)
# TODO is AWS_NO_SIGN_REQUEST necessary??
# Sys.setenv(AWS_NO_SIGN_REQUEST="YES")

# TODO create config yaml.

# TODO read in config yaml, must populate NA for items that are empty.

# Define input directory:
# TODO change this to reading the standardized metadata, not the generated data
raw_config <- yaml::read_yaml("/Users/guylitt/git/fsds/scripts/eval_ingest/xssa/xssa_schema.yaml")

datasets <- ds <- c("juliemai-xSSA",'all')[1] # A listing of datasets to grab attributes. Dataset names match what is inside dir_std_base.  'all' processes all datasets inside dir_std_base.
ds_nc_filenames <- c('juliemai-xSSA_Raven_blended.nc','*.nc')[1]


home_dir <- Sys.getenv("HOME")
dir_base <- file.path(home_dir,'noaa/regionalization/data')

dir_std_base <- file.path(dir_base,"input/user_data_std") # The location of standardized data generated by proc_fsds python package
dir_hydfab <- file.path(dir_base,'input','hydrofabric') # The local dir where hydrofabric data are stored to limit s3 connections
dir_db_attrs <- file.path(dir_base,'input','attributes') # The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}


s3_base <- "s3://lynker-spatial/tabular-resources" # s3 path containing hydrofabric-formatted attribute datasets
s3_bucket <- 'lynker-spatial' # s3 bucket containing hydrofabric data

s3_path_hydatl <- glue::glue('{s3_base}/hydroATLAS/hydroatlas_vars.parquet') # path to hydroatlas data formatted for hydrofabric

# Additional config options
hf_cat_sel <- c("total","all")[1] # total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
ext <- 'gpkg'
attr_sources <- c("hydroatlas","usgs") # "streamcat",
ha_vars <- c('pet_mm_s01', 'cly_pc_sav', 'cly_pc_uav') # hydroatlas variables
sc_vars <- c() # TODO look up variables. May need to select datasets first
usgs_vars <- c('TOT_TWI','TOT_PRSNOW','TOT_POPDENS90','TOT_EWT','TOT_RECHG') # list of variables retrievable using nhdplusTools::get_characteristics_metadata()
#-----------------------------------------------------


# 'all' an option if processing all datasets desired. Otherwise list datasets in config file
if(datasets=='all'){
  datasets <- list.dirs(dir_std_base,recursive=F)
}

# TODO generate this listing structure based on what is provided in yaml config & accounting for empty entries
# Note that if a path is provided, ensure the variable name includes 'path'. Same for directory having variable name with 'dir'
Retr_Params <- list(paths = list(dir_hydfab=dir_hydfab,
                                 dir_db_attrs=dir_db_attrs,
                              s3_path_hydatl = s3_path_hydatl),
                 vars = list(usgs_vars = usgs_vars,
                             ha_vars = ha_vars)
                 )

# Path checker/maker of anything that's a directory
for(dir in Retr_Params$paths){
  if(base::grepl('dir',dir)){
    if(!base::dir.exists(dir)){
      base::dir.create(dir)
    }
  }
}


for (dataset_name in datasets){

  # ----  Read in a standard format filename and file type from fsds_proc ---- #
  dir_ds <- base::file.path(dir_std_base,dataset_name)
  files_ds <- base::list.files(dir_ds)
  fns <- base::lapply(ds_nc_filenames,
                      function(x) files_ds[base::grep(x,files_ds)]) %>% unlist()

  if (base::any(base::grepl(".nc",fns))){ # Read in a netcdf file
    fn_nc <- fns[base::grep(".nc",fns)]
    if(length(fn_nc)!=1){
      stop(glue::glue("Expected that only one netcdf file exists in dir:\n{dir_ds}"))
    }
    dat_in <- file.path(dir_std_base,dataset_name,fn_nc)
    std_data <- tidync::tidync(dat_in)
  } else {
    stop("Create a different file format reader here.")
  }

  # TODO consider how different datasets will have different loc identifiers
  # Extract the gage ids
  gage_ids <- std_data$transforms$gage_id$gage_id
  featureSource <- 'nwissite' # TODO read in from std_data

  # ----------------------- Grab all needed attributes ----------------------- #
  ls_comid <- list()
  for (gage_id in gage_ids){
    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html
    site_id <- glue::glue('USGS-{gage_id}') # TODO perform this in fsds_proc

    nldi_feat <- list(featureSource =featureSource,
                 featureID = site_id)
    site_feature <- nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat)
    comid <- site_feature['comid']$comid
    ls_comid[[gage_id]] <- comid
    # Retrieve the variables corresponding to datasets of interest & update database
    loc_attrs <- proc.attr.hydfab::proc_attr_wrap(comid=comid,
                                                 Retr_Params=Retr_Params,
                                                 lyrs='network',overwrite=FALSE)
  }
  # --------------------------- Compile attributes --------------------------- #
  # Demonstration of how to retrieve attributes/comids that exist:
  # The comids of interest
  comids <- ls_comid %>% base::unname() %>% base::unlist()

  # The attribute variables of interest
  vars <- Retr_Params$vars %>% base::unlist() %>% base::unname()

  dat_all_attrs <- proc.attr.hydfab::retrieve_attr_exst(comids, vars,
                                       Retr_Params$paths$dir_db_attrs)
  rm(dat_all_attrs)
}
