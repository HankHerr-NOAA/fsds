# Functions to grab catchment attributes using hydrofabric and connected datasets

# Changelog / Contributions
#   2024-07-24 Originally created, GL


library(glue)
library(tidync)
library(dplyr)
library(arrow)
library(nhdplusTools)
library(hydrofabric)
library(hfsubsetR)

proc_attr_std_hfsub_name <- function(comid,custom_name='', ext='gpkg'){
  #' @title Standardidze hydrofabric subsetter's local filename
  #' @param comid the USGS common identifier, generated by nhdplusTools
  #' @param custom_name Desired custom name following 'hydrofab_'
  #' @param ext file extension of the hydrofrabric data. Default 'gpkg'
  #' @export
  hfsub_fn <- base::gsub(pattern = paste0(custom_name,"__"),
                         replacement = "_",
                         base::paste0('hydrofab_',custom_name,'_',comid,'.',ext))
  return(hfsub_fn)
}

proc_attr_hydatl <- function(hf_id, s3_path, ha_vars, local_path=NA){
  #' @title Retrieve hydroatlas variables
  #' @description retrieves hydrofabric variables from s3 bucket
  #' @param hf_id numeric. the hydrofabric id, expected to be the COMID
  #' @param s3_path character. full path to the s3 bucket's file holding the hydroatlas data
  #' @param ha_vars list of characters. The variables of interest in the hydroatlas v1
  #' @param local_path character. The local filepath where hydroatlas data are saved to reduce s3 bucket connections.
  #' @export
  # Reads in hydroatlas variables https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf

  # if(!is.numeric(hf_id)){
  #   warning(paste0("The hf_id ", hf_id, " expected to be numeric. Converting"))
  #   hf_id <- as.numeric(hf_id)
  # }

  # TODO check for local hydroatlas dataset before proceeding with s3 connection
  if(!base::is.na(local_path)){


  } else {
    ha <- arrow::open_dataset(s3_path) %>%
      dplyr::filter(hf_id %in% !!hf_id) %>%
      dplyr::select("hf_id", any_of(ha_vars)) %>%
      dplyr::collect()
  }

  if(!base::is.na(local_path)){
    # TODO generate standard hydroatlas filename

    # TODO write hydroatlas filename
  }
  return(ha)
}

proc_attr_usgs_nhd <- function(comid,usgs_vars){
  #' @title Retrieve USGS variables based on comid
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param usgs_vars list class. The standardized names of NHDplus variables.
  #' @seealso \code{nhdplusTools::get_characteristics_metadata() }
  #' @export
  # Get the s3 urls for each variable of interest
  usgs_meta<- nhdplusTools::get_characteristics_metadata() %>%
    dplyr::filter(ID %in% usgs_vars)
  # Extract the variable data corresponding to the COMID
  ls_usgs_mlti <- list()
  for (r in 1:nrow(usgs_meta)){
    var_id <- usgs_meta$ID[r]
    ls_usgs_mlti[[r]] <- arrow::open_dataset(usgs_meta$s3_url[r]) %>%
      dplyr::select(dplyr::all_of(c("COMID",var_id))) %>%
      dplyr::collect() %>%
      dplyr::filter(COMID==!!comid) #%>%
  }
  # Combining it all
  usgs_subvars <- ls_usgs_mlti %>% purrr::reduce(dplyr::full_join, by = 'COMID')
  return(usgs_subvars)
}


proc_attr_hf <- function(comid, dir_hydfab,custom_name="{lyrs}_",ext = 'gpkg',
                         lyrs=c('divides','network')[2],
                         hf_cat_sel=TRUE, overwrite=FALSE){

  #' @title Retrieve hydrofabric data of interest based on location identifier
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Checks to see if a local dataset exists. If not, retrieve from lynker-spatial s3 bucket
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param dir_hydfab character class. Local directory path for storing hydrofabric data
  #' @param custom_name character class. A custom name to insert into hydrofabric file. Default \code{glue("{lyrs}_")}
  #' @param ext character class. file extension of hydrofabric file. Default 'gpkg'
  #' @param lyrs character class. The layer name(s) of interest from hydrofabric. Default 'network'.
  #' @param hf_cat_sel boolean. TRUE for a total catchment characterization specific to a single comid, FALSE (or anything else) for all subcatchments
  #' @param overwrite boolean. Overwrite local data when pulling from hydrofabric s3 bucket? Default FALSE.
  #' @export

  # Build the hydfab filepath
  name_file <- proc.attr.hydfab::proc_attr_std_hfsub_name(comid=comid,
                                   custom_name=glue::glue('{lyrs}_'),
                                   ext=ext)
  fp_cat <- base::file.path(dir_hydfab, name_file)

  if(!base::dir.exists(dir_hydfab)){
    warning(glue::glue("creating the following directory: {dir_hydfab}"))
    base::dir.create(dir_hydfab)
  }

  # Generate the nldi feature listing
  nldi_feat <- list(featureSource ="comid",
                         featureID = comid)

  # Download hydrofabric file if it doesn't exist already
  # Utilize hydrofabric subsetter for the catchment and download to local path
  hfsubsetR::get_subset(nldi_feature = nldi_feat,outfile = fp_cat,
                        type = 'reference',lyrs = lyrs,
                        overwrite=overwrite)

  # Read the hydrofabric file gpkg for each layer
  hfab_ls <- list()
  if (ext == 'gpkg') {
    # Define layers
    layers <- sf::st_layers(dsn = fp_cat)
    for (lyr in layers$name){
      hfab_ls[[lyr]] <- sf::read_sf(fp_cat,layer=lyr)
    }
  } else {
    stop("# TODO add in the type of hydrofabric file to read based on extension")
  }

  net <- hfab_ls[[lyrs]] %>%
    dplyr::select(divide_id, hf_id) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(divide_id) %>% dplyr::slice(1)

  if (hf_cat_sel==TRUE){
    # interested in the single location's aggregated catchment data
    net <- net %>% base::subset(hf_id==base::as.numeric(comid))
  }
  return(net)
}


# TODO make attr_sources a structured list of lists following a standard format
proc_attr_wrap <- function(comid, Retr_Params, lyrs='network',overwrite=FALSE){
  #' @title Wrapper to retrieve variables when processing attributes
  #' @description Identifies a comid location using the hydrofabric and then
  #' processes user-requested variables from multiple sources
  #' @param comid character. The common identifier USGS location code for a surface water feature.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @export

  # Retrieve the hydrofabric id
  net <- proc.attr.hydfab::proc_attr_hf(comid=comid,dir_hydfab=Retr_Params$paths$dir_hydfab,
                      custom_name = "{lyrs}_",lyrs=lyrs,overwrite=overwrite)

  attr_data <- list()
  if ('ha_vars' %in% names(Retr_Params$vars)){
    # Hydroatlas variable query
    attr_data[['ha']] <- proc.attr.hydfab::proc_attr_hydatl(s3_path=Retr_Params$paths$s3_path_hydatl,
                                          hf_id=net$hf_id,
                                          ha_vars=Retr_Params$vars$ha_vars) %>%
                                # ensures 'COMID' exists as colname
                                dplyr::rename("COMID" = "hf_id")
  }
  if (base::any(base::grepl("usgs_vars", names(Retr_Params$vars)))){
    # USGS nhdplusv2 query
    attr_data[['usgs']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=net$hf_id,
                                                                usgs_vars=Retr_Params$vars$usgs_vars)
  }

  ########## May add more data sources here and append to attr_data ###########


  # Ensure consistent identifier formatting has been created before combining
  if (!base::all(base::unlist((base::lapply(attr_data, function(x) any(grepl("COMID", colnames(x)))))))){
    stop("Expecting 'COMID' as a column name identifier in every dataset")
  }
  # Combine all attr_data by column
  all_attr <- attr_data %>% purrr::reduce(dplyr::full_join, by = 'COMID')
  return(all_attr)
}
