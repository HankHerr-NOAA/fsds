# Functions to grab catchment attributes using hydrofabric and connected datasets

# Changelog / Contributions
#   2024-07-24 Originally created, GL


library(glue)
library(tidync)
library(dplyr)
library(arrow)
library(nhdplusTools)
library(hydrofabric)
library(hfsubsetR)
library(data.table)

proc_attr_std_hfsub_name <- function(comid,custom_name='', ext='gpkg'){
  #' @title Standardidze hydrofabric subsetter's local filename
  #' @param comid the USGS common identifier, generated by nhdplusTools
  #' @param custom_name Desired custom name following 'hydrofab_'
  #' @param ext file extension of the hydrofrabric data. Default 'gpkg'
  #' @export
  hfsub_fn <- base::gsub(pattern = paste0(custom_name,"__"),
                         replacement = "_",
                         base::paste0('hydrofab_',custom_name,'_',comid,'.',ext))
  return(hfsub_fn)
}

proc_attr_hydatl <- function(hf_id, s3_path, ha_vars, local_path=NA){
  #' @title Retrieve hydroatlas variables
  #' @description retrieves hydrofabric variables from s3 bucket
  #' @param hf_id numeric. the hydrofabric id, expected to be the COMID
  #' @param s3_path character. full path to the s3 bucket's file holding the hydroatlas data
  #' @param ha_vars list of characters. The variables of interest in the hydroatlas v1
  #' @param local_path character. The local filepath where hydroatlas data are saved to reduce s3 bucket connections.
  #' @export
  # Reads in hydroatlas variables https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf

  # if(!is.numeric(hf_id)){
  #   warning(paste0("The hf_id ", hf_id, " expected to be numeric. Converting"))
  #   hf_id <- as.numeric(hf_id)
  # }

  # TODO check for local hydroatlas dataset before proceeding with s3 connection
  if(!base::is.na(local_path)){


  } else {
    ha <- arrow::open_dataset(s3_path) %>%
      dplyr::filter(hf_id %in% !!hf_id) %>%
      dplyr::select("hf_id", any_of(ha_vars)) %>%
      dplyr::collect()
  }

  if(!base::is.na(local_path)){
    # TODO generate standard hydroatlas filename

    # TODO write hydroatlas filename
  }
  return(ha)
}

proc_attr_usgs_nhd <- function(comid,usgs_vars){
  #' @title Retrieve USGS variables based on comid
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param usgs_vars list class. The standardized names of NHDplus variables.
  #' @seealso \code{nhdplusTools::get_characteristics_metadata() }
  #' @export
  # Get the s3 urls for each variable of interest
  usgs_meta<- nhdplusTools::get_characteristics_metadata() %>%
    dplyr::filter(ID %in% usgs_vars)
  # Extract the variable data corresponding to the COMID
  ls_usgs_mlti <- list()
  for (r in 1:nrow(usgs_meta)){
    var_id <- usgs_meta$ID[r]
    ls_usgs_mlti[[r]] <- arrow::open_dataset(usgs_meta$s3_url[r]) %>%
      dplyr::select(dplyr::all_of(c("COMID",var_id))) %>%
      dplyr::collect() %>%
      dplyr::filter(COMID==!!comid) #%>%
  }
  # Combining it all
  usgs_subvars <- ls_usgs_mlti %>% purrr::reduce(dplyr::full_join, by = 'COMID')
  return(usgs_subvars)
}


proc_attr_hf <- function(comid, dir_hydfab,custom_name="{lyrs}_",ext = 'gpkg',
                         lyrs=c('divides','network')[2],
                         hf_cat_sel=TRUE, overwrite=FALSE){

  #' @title Retrieve hydrofabric data of interest based on location identifier
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Checks to see if a local dataset exists. If not, retrieve from lynker-spatial s3 bucket
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param dir_hydfab character class. Local directory path for storing hydrofabric data
  #' @param custom_name character class. A custom name to insert into hydrofabric file. Default \code{glue("{lyrs}_")}
  #' @param ext character class. file extension of hydrofabric file. Default 'gpkg'
  #' @param lyrs character class. The layer name(s) of interest from hydrofabric. Default 'network'.
  #' @param hf_cat_sel boolean. TRUE for a total catchment characterization specific to a single comid, FALSE (or anything else) for all subcatchments
  #' @param overwrite boolean. Overwrite local data when pulling from hydrofabric s3 bucket? Default FALSE.
  #' @export

  # Build the hydfab filepath
  name_file <- proc.attr.hydfab::proc_attr_std_hfsub_name(comid=comid,
                                   custom_name=glue::glue('{lyrs}_'),
                                   ext=ext)
  fp_cat <- base::file.path(dir_hydfab, name_file)

  if(!base::dir.exists(dir_hydfab)){
    warning(glue::glue("creating the following directory: {dir_hydfab}"))
    base::dir.create(dir_hydfab)
  }

  # Generate the nldi feature listing
  nldi_feat <- list(featureSource ="comid",
                         featureID = comid)

  # Download hydrofabric file if it doesn't exist already
  # Utilize hydrofabric subsetter for the catchment and download to local path
  hfsubsetR::get_subset(nldi_feature = nldi_feat,outfile = fp_cat,
                        type = 'reference',lyrs = lyrs,
                        overwrite=overwrite)

  # Read the hydrofabric file gpkg for each layer
  hfab_ls <- list()
  if (ext == 'gpkg') {
    # Define layers
    layers <- sf::st_layers(dsn = fp_cat)
    for (lyr in layers$name){
      hfab_ls[[lyr]] <- sf::read_sf(fp_cat,layer=lyr)
    }
  } else {
    stop("# TODO add in the type of hydrofabric file to read based on extension")
  }

  net <- hfab_ls[[lyrs]] %>%
    dplyr::select(divide_id, hf_id) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(divide_id) %>% dplyr::slice(1)

  if (hf_cat_sel==TRUE){
    # interested in the single location's aggregated catchment data
    net <- net %>% base::subset(hf_id==base::as.numeric(comid))
  }
  return(net)
}

proc_attr_exst_wrap <- function(comid,path_attrs,vars_ls,bucket_conn=NA){
  #' @title Existing attribute data checker
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Retrieves what attribute data already exists in a data storage
  #'  path for a given comid and identifies missing attributes.
  #'  Returns list of
  #'   - dt_all: a data.table of existing comid data,
  #'   - need_vars: a list of datasource ids containing a list of variable
  #'        names that will be downloaded.

  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param path_attrs character. Path to attribute file data storage location
  #' @param vars_ls list. Variable names
  #' @param bucket_conn TODO add cloud conn details in case data stored in s3
  #' @seealso [proc_attr_wrap()]
  #' @export
  #'
  # Changelog / Contributions
  #  2024-07-25 Originally created, GL

  # TODO adapt this check if stored in cloud (e.g. s3 connection checker)
  # Check that data has been created
  path_attrs_exst <- any(c(base::file.exists(path_attrs)))

  # Also make sure the directory exists:
  if(!dir.exists(base::dirname(path_attrs)) && is.na(bucket_conn)){
    dir.create(base::dirname(path_attrs))
  } # TODO adapt if stored in cloud (e.g. s3 connection checker)

  if(path_attrs_exst==TRUE){
    dt_all <- arrow::open_dataset(path_attrs) %>% data.table::as.data.table()
    need_vars <- list()
    for(var_srce in names(vars_ls)){
      # Compare/contrast what is there vs. desired
      attrs_reqd <- vars_ls[[var_srce]]
      attrs_needed <- attrs_reqd[which(!attrs_reqd %in% dt_all$attribute)]

      if(length(attrs_needed)>0){ # Only build list of variables needed
        need_vars[[var_srce]] <- attrs_needed
      }
    }
  } else {
    # No variable subsetting required. Grab them all for this comid
    need_vars <- vars_ls
    dt_all <- data.table::data.table() # to be populated.
  }
  return(list(dt_all=dt_all,need_vars=need_vars))
}

# TODO make attr_sources a structured list of lists following a standard format
proc_attr_wrap <- function(comid, Retr_Params, lyrs='network',overwrite=FALSE){
  #' @title Wrapper to retrieve variables when processing attributes
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Identifies a comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file. Re-processing runs only download data
  #' that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   COMID - USGS common identifier
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comid character. The common identifier USGS location code for a surface water feature.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @export

  # Changelog / Contributions
  #   2024-07-25 Originally created, GL

  # Retrieve the hydrofabric id
  net <- proc.attr.hydfab::proc_attr_hf(comid=comid,
                                        dir_hydfab=Retr_Params$paths$dir_hydfab,
                                        custom_name ="{lyrs}_",
                                        lyrs=lyrs,overwrite=overwrite)

  path_attrs <- base::file.path(Retr_Params$paths$dir_db_attrs,
                          base::paste0("comid_",comid,"_attrs.parquet"))
  vars_ls <- Retr_Params$vars


  # ----------- existing dataset checker ----------- #
  ls_chck <- proc.attr.hydfab::proc_attr_exst_wrap(comid,path_attrs,
                                                   vars_ls,bucket_conn=NA)
  dt_all <- ls_chck$dt_all
  need_vars <- ls_chck$need_vars

  # --------------- dataset grabber ---------------- #
  attr_data <- list()
  if ('ha_vars' %in% base::names(need_vars)){
    # Hydroatlas variable query; list name formatted as {dataset_name}__v{version_number}
    attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(s3_path=Retr_Params$paths$s3_path_hydatl,
                                          hf_id=net$hf_id,
                                          ha_vars=need_vars$ha_vars) %>%
                                # ensures 'COMID' exists as colname
                                dplyr::rename("COMID" = "hf_id")
  }
  if (base::any(base::grepl("usgs_vars", base::names(need_vars)))){
    # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{version_number}
    attr_data[['usgs_nhdplus__v2']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=net$hf_id,
                                                                usgs_vars=need_vars$usgs_vars)
  }

  ########## May add more data sources here and append to attr_data ###########


  # ----------- dataset standardization ------------ #
  if (!base::all(base::unlist((
          base::lapply(attr_data, function(x)
                  base::any(base::grepl("COMID", colnames(x)))))))){
    stop("Expecting 'COMID' as a column name identifier in every dataset")
  }
  # Ensure consistent format of dataset
  attr_data_ls <- list()
  for(dat_srce in base::names(attr_data)){
    sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
    sub_dt_dat$COMID <- base::as.character(sub_dt_dat$COMID)
    sub_dt_dat$data_source <- base::as.character(dat_srce)
    sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
      base::format(Sys.time()),tz="UTC"))
    # Convert from wide to long format
    attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
                             id.vars = c('COMID','data_source','dl_timestamp'),
                             variable.name = 'attribute')
  }
  # Combine freshly-acquired data
  dt_new_dat <- data.table::rbindlist(attr_data_ls)

  # Combined dt of existing data and newly acquired data
  if(base::dim(dt_all)[1]>0 && base::dim(dt_new_dat)[1]>0){
    dt_cmbo <- data.table::merge.data.table(dt_all,dt_new_dat,
                                            all=TRUE,no.dups=TRUE)
  } else if (base::dim(dt_new_dat)[1] >0){
    dt_cmbo <- dt_new_dat
  } else {
    dt_cmbo <- dt_all
  }

  # Write attribute variable data specific to a comid here
  arrow::write_parquet(dt_cmbo,path_attrs)

  return(dt_cmbo)
}
