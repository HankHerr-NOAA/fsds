# setup for the Julie Mai xSSA datasets from 2022 Nature Comm pub
col_schema:   # required column mappings in the evaluation metrics dataset
  - 'featureID': 'USGS-{gage_id}' # python f-string / R glue() format; converting the 'gage_id' to the standardized featureID used by nhdplusTools. Must use '{gage_id}' e.g. 'USGS-{gage_id}'
  - 'featureSource': 'nwissite' # The standardized nhdplusTools featureSource. Possible featureSources might be 'nwissite', 'comid'.
file_io: # May define {home_dir} for python's '{home_dir}/string_path'.format(home_dir =str(Path.home())) functionality
  - 'dir_save': '{home_dir}/noaa/regionalization/data/input'
  - 'save_type': 'netcdf' #  Required. Use 'csv' to create a directory structure & save multiple files. May also save as hierarchical files 'netcdf' or 'zarr', or if 'csv' chosen, a directory structure is created
  - 'save_loc': 'local' #  Required.  Use 'local' for saving to a local path via dir_save. Future work will create an approach for 'aws' or other cloud saving methods
  - 'dir_base' : '{home_dir}/noaa/regionalization/data/input' # Required. The save location of standardized output
  - 'dir_std_base' : '{dir_base}/user_data_std' # Required. The location of standardized data generated by proc_fsds python package
  - 'dir_db_hydfab' : '{dir_base}/hydrofabric' # Required. The local dir where hydrofabric data are stored (limits the total s3 connections)
  - 'dir_db_attrs' : '{dir_base}/attributes' # Required. The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}
formulation_metadata:  
  - 'datasets': # Required. Must match directory name inside dir_std_base. May be a list of items, or simply 'all' to select everything inside dir_std_base.
    - 'juliemai-xSSA' 
  # - 'ds_nc_filenames': '*.nc' # Required. A string that should match the netcdf filename for the standardized dataset generated by fsds_proc.
  - 'formulation_base': 'Raven_blended' # Informational. Unique name of formulation.
hydfab_config:
 - 's3_base' : "s3://lynker-spatial/tabular-resources" # Required. s3 path containing hydrofabric-formatted attribute datasets
 - 's3_bucket' : 'lynker-spatial' # Required. s3 bucket containing hydrofabric data
 - 'ext' : 'gpkg' # Required. file extension of the hydrofrabric data. Default 'gpkg'. 
 - 'hf_cat_sel': "total" # Required. Options include 'total' or 'all'; total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
attr_select: # The names of variable sublistings are standardized, e.g. ha_vars, usgs_vars, sc_vars
  - 's3_path_hydatl' : '{s3_base}/hydroATLAS/hydroatlas_vars.parquet' # path to hydroatlas data formatted for hydrofabric. Required only if hydroatlas variables desired.
  - 'ha_vars':  # hydroatlas variables. Must specify s3_path_hydatl if desired.
    - 'pet_mm_s01'
    - 'cly_pc_sav'
    - 'cly_pc_uav'
  - 'usgs_vars': # list of variables retrievable using nhdplusTools::get_characteristics_metadata(). 
    - 'TOT_TWI'
    - 'TOT_PRSNOW'
    - 'TOT_POPDENS90'
    - 'TOT_EWT'
    - 'TOT_RECHG'
  - 'sc_vars':
    -
references: # All optional but **very** helpful metadata
  - 'input_filepath': '{base_dir}/julemai-xSSA/data_in/basin_metadata/basin_validation_results.txt'
  - 'source_url': 'https://zenodo.org/records/5730428'
  - 'dataset_doi': '10.5281/zenodo.5730428'
  - 'literature_doi': 'https://doi.org/10.1038/s41467-022-28010-7'
